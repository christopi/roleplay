{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil/miniconda3/envs/rev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import bittensor as bt\n",
    "import pydantic\n",
    "from starlette.types import Send\n",
    "from starlette.responses import Response, StreamingResponse\n",
    "from functools import partial\n",
    "from typing import Callable, Awaitable, List, Tuple\n",
    "import asyncio\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "bt.debug()\n",
    "\n",
    "\n",
    "# This is a subclass of StreamingSynapse for prompting network functionality\n",
    "class StreamPrompting(bt.StreamingSynapse):\n",
    "    \"\"\"\n",
    "    StreamPrompting is a subclass of StreamingSynapse that is specifically designed for prompting network functionality.\n",
    "    It overrides abstract methods from the parent class to provide concrete implementations for processing streaming responses,\n",
    "    deserializing the response, and extracting JSON data.\n",
    "\n",
    "    Attributes:\n",
    "        roles: List of roles associated with the prompt.\n",
    "        messages: List of messages to be processed.\n",
    "        completion: A string to store the completion result.\n",
    "    \"\"\"\n",
    "\n",
    "    roles: List[str] = pydantic.Field(\n",
    "        ...,\n",
    "        title=\"Roles\",\n",
    "        description=\"A list of roles in the Prompting scenario. Immuatable.\",\n",
    "        allow_mutation=False,\n",
    "    )\n",
    "\n",
    "    messages: List[str] = pydantic.Field(\n",
    "        ...,\n",
    "        title=\"Messages\",\n",
    "        description=\"A list of messages in the Prompting scenario. Immutable.\",\n",
    "        allow_mutation=False,\n",
    "    )\n",
    "\n",
    "    completion: str = pydantic.Field(\n",
    "        \"\",\n",
    "        title=\"Completion\",\n",
    "        description=\"Completion status of the current Prompting object. This attribute is mutable and can be updated.\",\n",
    "    )\n",
    "\n",
    "    async def process_streaming_response(self, response):\n",
    "        \"\"\"\n",
    "        Asynchronously processes chunks of a streaming response, decoding the chunks from utf-8 to strings \n",
    "        and appending them to the `completion` attribute. The primary goal of this method is to accumulate the \n",
    "        content from the streaming response in a sequential manner.\n",
    "\n",
    "        This method is particularly vital when the streaming response from the server is broken down into multiple \n",
    "        chunks, and a comprehensive result needs to be constructed from these individual chunks.\n",
    "\n",
    "        Args:\n",
    "            response: The response object from which the streamed content is fetched. This content typically \n",
    "                    contains chunks of string data that are being streamed from the server.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If there is an issue decoding the streamed chunks.\n",
    "\n",
    "        Note:\n",
    "            This method is designed for utf-8 encoded strings. If the streamed content has a different encoding, \n",
    "            it may need to be adjusted accordingly.\n",
    "        \"\"\"\n",
    "        if self.completion is None:\n",
    "            self.completion = \"\"\n",
    "        async for chunk in response.content.iter_any():\n",
    "            tokens = chunk.decode('utf-8').split('\\n')\n",
    "            for token in tokens:\n",
    "                if token:\n",
    "                    self.completion += token\n",
    "\n",
    "    def deserialize(self):\n",
    "        \"\"\"\n",
    "        Deserializes the response by returning the completion attribute.\n",
    "\n",
    "        Returns:\n",
    "            str: The completion result.\n",
    "        \"\"\"\n",
    "        return self.completion\n",
    "\n",
    "    def extract_response_json(self, response):\n",
    "        \"\"\"\n",
    "        Extracts various components of the response object, including headers and specific information related \n",
    "        to dendrite and axon, into a structured JSON format. This method aids in simplifying the raw response \n",
    "        object into a format that's easier to read and interpret.\n",
    "\n",
    "        The method is particularly useful for extracting specific metadata from the response headers which \n",
    "        provide insights about the response or the server's configurations. Moreover, details about dendrite \n",
    "        and axon extracted from headers can provide information about the neural network layers that were \n",
    "        involved in the request-response cycle.\n",
    "\n",
    "        Args:\n",
    "            response: The response object, typically an instance of an HTTP response, containing the headers \n",
    "                    and the content that needs to be extracted.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the structured data extracted from the response object. This includes \n",
    "                data such as the server's name, timeout details, data sizes, and information about dendrite \n",
    "                and axon among others.\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If expected headers or response components are missing.\n",
    "\n",
    "        Note:\n",
    "            This method assumes a certain structure and naming convention for the headers. If the server \n",
    "            changes its header naming convention or structure, this method may need adjustments.\n",
    "        \"\"\"\n",
    "        headers = {k.decode('utf-8'): v.decode('utf-8') for k, v in response.__dict__[\"_raw_headers\"]}\n",
    "\n",
    "        def extract_info(prefix):\n",
    "            return {key.split('_')[-1]: value for key, value in headers.items() if key.startswith(prefix)}\n",
    "\n",
    "        return {\n",
    "            \"name\": headers.get('name', ''),\n",
    "            \"timeout\": float(headers.get('timeout', 0)),\n",
    "            \"total_size\": int(headers.get('total_size', 0)),\n",
    "            \"header_size\": int(headers.get('header_size', 0)),\n",
    "            \"dendrite\": extract_info('bt_header_dendrite'),\n",
    "            \"axon\": extract_info('bt_header_axon'),\n",
    "            \"roles\": self.roles,\n",
    "            \"messages\": self.messages,\n",
    "            \"completion\": self.completion,\n",
    "        }\n",
    "\n",
    "# This should encapsulate all the logic for generating a streaming response\n",
    "def prompt(synapse: StreamPrompting) -> StreamPrompting:\n",
    "    \"\"\"\n",
    "    Generates a streaming response for the provided synapse.\n",
    "\n",
    "    This function serves as the main entry point for handling streaming prompts. It takes\n",
    "    the incoming synapse which contains messages to be processed and returns a streaming\n",
    "    response. The function uses the GPT-2 tokenizer and a simulated model to tokenize and decode\n",
    "    the incoming message, and then sends the response back to the client token by token.\n",
    "\n",
    "    Args:\n",
    "        synapse (StreamPrompting): The incoming StreamPrompting instance containing the messages to be processed.\n",
    "\n",
    "    Returns:\n",
    "        StreamPrompting: The streaming response object which can be used by other functions to\n",
    "                        stream back the response to the client.\n",
    "\n",
    "    Usage:\n",
    "        This function can be extended and customized based on specific requirements of the\n",
    "        miner. Developers can swap out the tokenizer, model, or adjust how streaming responses\n",
    "        are generated to suit their specific applications.\n",
    "    \"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Simulated function to decode token IDs into strings. In a real-world scenario,\n",
    "    # this can be replaced with an actual model inference step.\n",
    "    def model(ids):\n",
    "        return (tokenizer.decode(id) for id in ids)\n",
    "\n",
    "    async def _prompt(text: str, send: Send):\n",
    "        \"\"\"\n",
    "        Asynchronously processes the input text and sends back tokens as a streaming response.\n",
    "\n",
    "        This function takes an input text, tokenizes it using the GPT-2 tokenizer, and then\n",
    "        uses the simulated model to decode token IDs into strings. It then sends each token\n",
    "        back to the client as a streaming response, with a delay between tokens to simulate\n",
    "        the effect of real-time streaming.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text message to be processed.\n",
    "            send (Send): An asynchronous function that allows sending back the streaming response.\n",
    "\n",
    "        Usage:\n",
    "            This function can be adjusted based on the streaming requirements, speed of\n",
    "            response, or the model being used. Developers can also introduce more sophisticated\n",
    "            processing steps or modify how tokens are sent back to the client.\n",
    "        \"\"\"\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.squeeze()\n",
    "        buffer = []\n",
    "        N = 4  # Number of tokens to send back to the client at a time\n",
    "        for token in model(input_ids):\n",
    "            buffer.append(token)\n",
    "            # If buffer has N tokens, send them back to the client.\n",
    "            if len(buffer) == N:\n",
    "                await asyncio.sleep(0.3) # Sleep just to show streaming effect\n",
    "                joined_buffer = \"\".join(buffer)\n",
    "                await send(\n",
    "                    {\n",
    "                        \"type\": \"http.response.body\",\n",
    "                        \"body\": joined_buffer.encode(\"utf-8\"),\n",
    "                        \"more_body\": True,\n",
    "                    }\n",
    "                )\n",
    "                bt.logging.debug(f\"Streamed tokens: {joined_buffer}\")\n",
    "                buffer = []  # Clear the buffer for next batch of tokens\n",
    "\n",
    "        # Send any remaining tokens in the buffer\n",
    "        if buffer:\n",
    "            joined_buffer = \"\".join(buffer)\n",
    "            await send(\n",
    "                {\n",
    "                    \"type\": \"http.response.body\",\n",
    "                    \"body\": joined_buffer.encode(\"utf-8\"),\n",
    "                    \"more_body\": False,  # No more tokens to send\n",
    "                }\n",
    "            )\n",
    "            bt.logging.trace(f\"Streamed tokens: {joined_buffer}\")\n",
    "\n",
    "    message = synapse.messages[0]\n",
    "    token_streamer = partial(_prompt, message)\n",
    "    return synapse.create_streaming_response(token_streamer)\n",
    "\n",
    "def blacklist(synapse: StreamPrompting) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Determines whether the synapse should be blacklisted.\n",
    "\n",
    "    Args:\n",
    "        synapse: A StreamPrompting instance.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[bool, str]: Always returns False, indicating that the synapse should not be blacklisted.\n",
    "    \"\"\"\n",
    "    return False, \"\"\n",
    "\n",
    "def priority(synapse: StreamPrompting) -> float:\n",
    "    \"\"\"\n",
    "    Determines the priority of the synapse.\n",
    "\n",
    "    Args:\n",
    "        synapse: A StreamPrompting instance.\n",
    "\n",
    "    Returns:\n",
    "        float: Always returns 0.0, indicating the default priority.\n",
    "    \"\"\"\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dendrite(5C86aJ2uQawR6P6veaJQXNK9HaWh6NMbUhTiLs65kq4ZW3NH)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an Axon instance on port 8099.\n",
    "axon = bt.axon(port=8099)\n",
    "\n",
    "# Attach the forward, blacklist, and priority functions to the Axon.\n",
    "# forward_fn: The function to handle forwarding logic.\n",
    "# blacklist_fn: The function to determine if a request should be blacklisted.\n",
    "# priority_fn: The function to determine the priority of the request.\n",
    "axon.attach(\n",
    "    forward_fn=prompt,\n",
    "    blacklist_fn=blacklist,\n",
    "    priority_fn=priority\n",
    ")\n",
    "\n",
    "# Start the Axon to begin listening for requests.\n",
    "axon.start()\n",
    "\n",
    "# Create a Dendrite instance to handle client-side communication.\n",
    "d = bt.dendrite()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2023-09-28 20:17:08.264\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | dendrite | --> | 4377 B | StreamPrompting | 5C86aJ2uQawR6P6veaJQXNK9HaWh6NMbUhTiLs65kq4ZW3NH | 149.137.225.62:8099 | 0 | Success\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2023-09-28 20:17:08.470\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | axon     | <-- | 1334 B | StreamPrompting | 5C86aJ2uQawR6P6veaJQXNK9HaWh6NMbUhTiLs65kq4ZW3NH | 127.0.0.1:36262 | 200 | Success \n",
      "\u001b[34m2023-09-28 20:17:08.670\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | axon     | --> | -1 B | StreamPrompting | 5C86aJ2uQawR6P6veaJQXNK9HaWh6NMbUhTiLs65kq4ZW3NH | 127.0.0.1:36262  | 200 | Success\n",
      "\u001b[34m2023-09-28 20:17:09.109\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: hello this is a\n",
      "\u001b[34m2023-09-28 20:17:09.611\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  test of a streaming\n",
      "\u001b[34m2023-09-28 20:17:10.114\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  response. Lorem\n",
      "\u001b[34m2023-09-28 20:17:10.614\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  ipsum d     \n",
      "\u001b[34m2023-09-28 20:17:11.115\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: olor sit amet\n",
      "\u001b[34m2023-09-28 20:17:11.616\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: , consectet  \n",
      "\u001b[34m2023-09-28 20:17:12.119\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: ur adipiscing\n",
      "\u001b[34m2023-09-28 20:17:12.621\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  elit, sed   \n",
      "\u001b[34m2023-09-28 20:17:13.124\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  do eiusmod  \n",
      "\u001b[34m2023-09-28 20:17:13.625\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  tempor incidid\n",
      "\u001b[34m2023-09-28 20:17:14.126\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: unt ut labore\n",
      "\u001b[34m2023-09-28 20:17:14.628\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  et dolore   \n",
      "\u001b[34m2023-09-28 20:17:15.130\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  magna aliqu \n",
      "\u001b[34m2023-09-28 20:17:15.632\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: a. Ut en     \n",
      "\u001b[34m2023-09-28 20:17:16.135\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: im ad minim ven\n",
      "\u001b[34m2023-09-28 20:17:16.636\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: iam, quis    \n",
      "\u001b[34m2023-09-28 20:17:17.138\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  nostrud exerc\n",
      "\u001b[34m2023-09-28 20:17:17.641\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: itation ullam\n",
      "\u001b[34m2023-09-28 20:17:18.144\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: co laboris n \n",
      "\u001b[34m2023-09-28 20:17:18.646\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: isi ut aliqu \n",
      "\u001b[34m2023-09-28 20:17:19.149\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: ip ex ea     \n",
      "\u001b[34m2023-09-28 20:17:19.650\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  commodo consequat\n",
      "\u001b[34m2023-09-28 20:17:20.152\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: . Duis a     \n",
      "\u001b[34m2023-09-28 20:17:20.515\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | dendrite | <-- | 4841 B | StreamPrompting | 5C86aJ2uQawR6P6veaJQXNK9HaWh6NMbUhTiLs65kq4ZW3NH | 149.137.225.62:8099 | 408 | Timedout after 12 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello this is a test of a streaming response. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis a']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2023-09-28 20:17:20.654\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: ute irure d  \n",
      "\u001b[34m2023-09-28 20:17:21.155\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: olor in repre\n",
      "\u001b[34m2023-09-28 20:17:21.658\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: henderit in  \n",
      "\u001b[34m2023-09-28 20:17:22.161\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  voluptate vel\n",
      "\u001b[34m2023-09-28 20:17:22.663\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: it esse c    \n",
      "\u001b[34m2023-09-28 20:17:23.165\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: illum dol    \n",
      "\u001b[34m2023-09-28 20:17:23.667\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: ore eu fug   \n",
      "\u001b[34m2023-09-28 20:17:24.169\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: iat nulla par\n",
      "\u001b[34m2023-09-28 20:17:24.670\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: iatur. Except\n",
      "\u001b[34m2023-09-28 20:17:25.173\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: eur sint occ \n",
      "\u001b[34m2023-09-28 20:17:25.674\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: aecat cup    \n",
      "\u001b[34m2023-09-28 20:17:26.176\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: idatat non   \n",
      "\u001b[34m2023-09-28 20:17:26.678\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  proident, s \n",
      "\u001b[34m2023-09-28 20:17:27.182\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: unt in culpa \n",
      "\u001b[34m2023-09-28 20:17:27.684\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens:  qui officia des\n",
      "\u001b[34m2023-09-28 20:17:28.186\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: erunt moll   \n",
      "\u001b[34m2023-09-28 20:17:28.689\u001b[0m | \u001b[34m\u001b[1m     DEBUG      \u001b[0m | Streamed tokens: it anim id est\n"
     ]
    }
   ],
   "source": [
    "# Send a request to the Axon using the Dendrite, passing in a StreamPrompting instance with roles and messages.\n",
    "# The response is awaited, as the Dendrite communicates asynchronously with the Axon.\n",
    "resp = await d(\n",
    "    [axon],\n",
    "    StreamPrompting(roles=[\"user\"], messages=[\"hello this is a test of a streaming response. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"])\n",
    ")\n",
    "\n",
    "# The response object contains the result of the streaming operation.\n",
    "resp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
